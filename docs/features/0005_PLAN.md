# Feature 0005: Unified PDF Processing Architecture

## Description

Refactor the PDF upload and processing workflow to create a unified, robust architecture that handles both URL-sourced PDFs and direct file uploads through a single processing pipeline. Move PDF upload testing to the authenticated dashboard environment to enable rapid iteration without reCAPTCHA friction. (Leave the front-end in place for now, we will later on connect the FE upload to this pipeline.)

## Current Architecture Issues

### Dual Processing Paths
- **ParseQueue** (`server/src/services/parseQueue.ts`): Handles URL-based processing through `restaurantMenuSources` table with queued jobs
- **Public Upload** (`server/src/api.ts` lines 103-263): Direct file processing with immediate parsing, stores base64 in `analysisData.fileContent`
- **File Storage**: PDF content stored as base64 strings in database rather than proper file storage
- **Mock Data**: `parseDigitalPdf()` in `pdfParser.ts` returns hardcoded French café menu instead of parsing actual content

### Processing Inconsistencies
- ParseQueue uses `ParseJob` interface with `restaurantMenuSourceId` reference
- Public uploads bypass queue system entirely and process immediately
- Different status tracking between `restaurantMenuSources.status` and `menuUploads.status`
- No unified document metadata or file persistence strategy

## Phase 1: Dashboard Integration (Immediate Testing Relief)

### MenuInsights Page Integration
- **File**: `ui/src/pages/MenuInsights.tsx`
- **Modify**: Add PDF upload functionality to existing `MenuUpload` component integration (lines 137-144)
- **Leverage**: Existing auth context (`useAuth()` hook) and `api.getUserMenus()` functionality

### Authenticated Upload Endpoint
- **File**: `server/src/api.ts`
- **Create**: New authenticated endpoint `/api/upload-menu` using `authMiddleware`
- **Reuse**: Existing PDF processing logic from public endpoint (lines 168-180)
- **Benefits**: Skip reCAPTCHA verification, direct user association via `c.get('user')`

### Menu Schema Utilization
- **Table**: `menuUploads` already supports authenticated uploads (`userId` field)
- **Remove**: Dependency on `publicUploads` table for authenticated flows
- **Status**: Direct `menuUploads.status` tracking instead of dual table approach

## Phase 2: Document Triage Service

### Document Triage Service with Content Detection
- **File**: `server/src/services/documentTriage.ts` (new)
- **Purpose**: Unified entry point that determines document type via content sniffing and routes to appropriate processing
- **Dependencies**: `pnpm add poppler-utils pdfinfo` (for PDF content analysis)
- **Interface**:
  ```typescript
  interface DocumentInput {
    type: 'url' | 'file';
    source: string | { content: string; mimeType: string };
    userId?: string;
    restaurantId?: string;
  }
  
  interface DocumentTriageResult {
    documentId: string;
    documentType: DocumentType; // Detected via content, not assumed
    processingStrategy: ParseStrategy;
    storageLocation: string;
    contentAnalysis: {
      textRatio: number; // % of page that is extractable text
      pageCount: number;
      hasImages: boolean;
      confidence: number; // Confidence in document type detection
    };
  }
  
  // Content-based detection logic
  async function detectDocumentType(filePath: string): Promise<DocumentType> {
    if (path.extname(filePath) === '.pdf') {
      // Use pdfinfo to get metadata
      const pdfInfo = await execAsync(`pdfinfo "${filePath}"`);
      
      // Extract text from first page and calculate text ratio
      const firstPageText = await execAsync(`pdftotext -f 1 -l 1 "${filePath}" -`);
      const textRatio = calculateTextRatio(firstPageText, filePath);
      
      // If <30% text content, classify as scanned
      return textRatio < 0.3 ? 'scanned_pdf' : 'digital_pdf';
    }
    
    // HTML detection logic for dynamic vs static
    if (filePath.includes('.html') || mimeType === 'text/html') {
      const content = await fs.readFile(filePath, 'utf-8');
      return content.includes('<script') ? 'html_dynamic' : 'html_static';
    }
    
    throw new Error(`Unsupported file type: ${filePath}`);
  }
  ```

### File Storage Service
- **File**: `server/src/services/fileStorage.ts` (new)
- **Implementation**: Local filesystem initially (`/uploads/documents/{userId}/{documentId}.pdf`)
- **Interface**:
  ```typescript
  async function storeDocument(content: string, userId: string, filename: string): Promise<string>
  async function retrieveDocument(documentPath: string): Promise<Buffer>
  ```

### Documents Table with State Machine
- **File**: `server/src/schema/documents.ts` (new)
- **Purpose**: Universal document tracking for both URL and file sources
- **Schema**:
  ```typescript
  documents = appSchema.table('documents', {
    id: text('id').primaryKey(),
    userId: text('user_id').references(() => users.id),
    originalName: text('original_name'),
    mimeType: text('mime_type'),
    fileSize: integer('file_size'),
    storagePath: text('storage_path'), // Local path or URL
    sourceType: text('source_type'), // 'upload' | 'url'
    sourceUrl: text('source_url'), // Original URL if applicable
    documentType: text('document_type'), // Detected via content sniffing
    
    // State machine status
    status: text('status').notNull().default('uploaded'), 
    // 'uploaded' → 'queued' → 'parsing' → 'parsed' → 'analyzing' → 'analyzed' → 'done'
    // Failed states: 'failed_parsing', 'failed_analysis'
    statusReason: text('status_reason'), // Human-readable reason for failures
    
    createdAt: timestamp('created_at').defaultNow()
  });
  
  // Separate tables for parsing and analysis lineage
  parseRuns = appSchema.table('parse_runs', {
    id: text('id').primaryKey(),
    documentId: text('document_id').references(() => documents.id),
    parserVersion: text('parser_version').notNull(), // v1.0.0, v1.1.0, etc.
    parseMethod: text('parse_method').notNull(), // 'pdf_digital', 'pdf_ocr', 'html'
    status: text('status').notNull(), // 'running', 'completed', 'failed'
    confidence: integer('confidence'), // 0-100
    rawOutput: jsonb('raw_output'), // Parsed text, layout data, etc.
    metadata: jsonb('metadata'), // Parser-specific metadata
    errorMessage: text('error_message'),
    startedAt: timestamp('started_at').defaultNow(),
    completedAt: timestamp('completed_at')
  });
  
  analysisRuns = appSchema.table('analysis_runs', {
    id: text('id').primaryKey(),
    parseRunId: text('parse_run_id').references(() => parseRuns.id),
    analysisVersion: text('analysis_version').notNull(), // v1.0.0, v1.1.0, etc.
    status: text('status').notNull(), // 'running', 'completed', 'failed'
    analysisResults: jsonb('analysis_results'), // Menu items, categories, metrics
    metrics: jsonb('metrics'), // Profitability, readability scores, etc.
    errorMessage: text('error_message'),
    startedAt: timestamp('started_at').defaultNow(),
    completedAt: timestamp('completed_at')
  });
  ```

## Phase 3: ParseQueue Enhancement

### State Machine Implementation
- **File**: `server/src/services/stateMachine.ts` (new)
- **Purpose**: Enforce valid state transitions and provide clear error handling
- **Implementation**:
  ```typescript
  type DocumentStatus = 
    | 'uploaded' | 'queued' | 'parsing' | 'parsed' 
    | 'analyzing' | 'analyzed' | 'done'
    | 'failed_parsing' | 'failed_analysis';

  const validTransitions: Record<DocumentStatus, DocumentStatus[]> = {
    uploaded: ['queued', 'failed_parsing'],
    queued: ['parsing', 'failed_parsing'],
    parsing: ['parsed', 'failed_parsing'],
    parsed: ['analyzing', 'failed_analysis'],
    analyzing: ['analyzed', 'failed_analysis'],
    analyzed: ['done'],
    done: ['analyzing'], // Allow re-analysis
    failed_parsing: ['queued'], // Allow retry
    failed_analysis: ['analyzing'] // Allow re-analysis
  };

  async function transitionDocumentStatus(
    documentId: string, 
    newStatus: DocumentStatus, 
    reason?: string
  ): Promise<void> {
    const current = await getCurrentStatus(documentId);
    if (!validTransitions[current].includes(newStatus)) {
      throw new Error(`Invalid transition from ${current} to ${newStatus}`);
    }
    
    await updateDocumentStatus(documentId, newStatus, reason);
  }
  ```

### Enhanced ParseJob Interface
- **File**: `server/src/types/url-parsing.ts`
- **Extend**: `ParseJob` interface to support both URL and file-based jobs
- **Changes**:
  ```typescript
  interface ParseJob {
    id: string;
    documentId: string; // Reference to documents table
    runId: string; // Reference to parse_runs table  
    inputType: 'url' | 'file';
    inputSource: string; // URL or file path
    parserVersion: string; // v1.0.0, v1.1.0, etc.
    createdAt: Date;
    userId?: string;
    restaurantId?: string;
  }
  
  interface AnalysisJob {
    id: string;
    parseRunId: string; // Reference to completed parse run
    analysisVersion: string; // v1.0.0, v1.1.0, etc.
    createdAt: Date;
  }
  ```

### Separate ParseQueue and AnalysisQueue
- **File**: `server/src/services/parseQueue.ts`
- **Changes**: Focus solely on parsing, remove analysis logic
- **Method**: `processParseJob()` 
  - Use `documents` and `parse_runs` tables instead of `restaurantMenuSources`
  - Update document status using state machine: `queued` → `parsing` → `parsed`
  - Store raw parsing output in `parse_runs.rawOutput` without analysis
- **New Method**: `enqueueParseJob(documentId: string, parserVersion: string): Promise<string>`

- **File**: `server/src/services/analysisQueue.ts` (new)
- **Purpose**: Separate queue for analysis jobs that consume parse results
- **Method**: `processAnalysisJob()`
  - Takes `parseRunId` as input, reads from `parse_runs.rawOutput`
  - Performs menu item extraction, categorization, metrics calculation
  - Stores results in `analysis_runs.analysisResults`
  - Updates document status: `parsed` → `analyzing` → `analyzed` → `done`

### Parser Router
- **File**: `server/src/services/parseRouter.ts` (new)
- **Purpose**: Route documents to appropriate parsers based on content type and document type
- **Logic**:
  ```typescript
  function routeToParser(documentType: DocumentType, mimeType: string): ParseStrategy {
    if (mimeType === 'application/pdf') {
      return documentType === 'scanned_pdf' ? 'pdf_ocr' : 'pdf_digital';
    }
    if (mimeType === 'text/html') {
      return documentType === 'html_dynamic' ? 'javascript' : 'html';
    }
    throw new Error(`Unsupported document type: ${documentType}`);
  }
  ```

## Phase 4: Layout-Aware PDF Parser Implementation

### Dependencies Installation
- **Command**: `pnpm add pdfjs-dist tesseract.js poppler-utils`
- **Purpose**: `pdfjs-dist` for layout-aware parsing, `tesseract.js` for OCR, `poppler-utils` for PDF utilities
- **Note**: Remove `pdf-parse` (orderless text) and `sharp` (doesn't render PDFs)

### Layout-Aware Digital PDF Parser
- **File**: `server/src/services/parsers/pdfParser.ts`
- **Replace**: Mock implementation with layout-aware parsing
- **Implementation**:
  ```typescript
  import * as pdfjs from 'pdfjs-dist/legacy/build/pdf';
  import { execAsync } from 'child_process';
  
  interface TextItem {
    text: string;
    x: number;
    y: number;
    width: number;
    height: number;
    fontName: string;
    fontSize: number;
  }
  
  async function parseDigitalPdf(filePath: string): Promise<ParseResult> {
    // Use PDF.js for layout-aware extraction
    const doc = await pdfjs.getDocument(filePath).promise;
    const page = await doc.getPage(1); // Start with first page
    const textContent = await page.getTextContent();
    
    // Extract positioned text items
    const textItems: TextItem[] = textContent.items.map(item => ({
      text: item.str,
      x: item.transform[4],
      y: item.transform[5], 
      width: item.width,
      height: item.height,
      fontName: item.fontName,
      fontSize: item.transform[0]
    }));
    
    // Build heuristics for menu structure
    const structuredContent = buildMenuStructure(textItems);
    
    // Fallback to pure text if layout parsing fails
    if (structuredContent.confidence < 0.6) {
      console.warn('Layout parsing failed, falling back to text extraction');
      const textOnly = await execAsync(`pdftotext "${filePath}" -`);
      return parseMenuFromText(textOnly);
    }
    
    return {
      success: true,
      rawOutput: {
        textItems,
        structuredContent,
        layoutMetadata: {
          pageWidth: page.getViewport({ scale: 1.0 }).width,
          pageHeight: page.getViewport({ scale: 1.0 }).height
        }
      },
      parseMethod: 'pdf_digital_layout',
      confidence: structuredContent.confidence
    };
  }
  
  function buildMenuStructure(textItems: TextItem[]) {
    // Heuristics for menu layout:
    // 1. Group items by Y-coordinate proximity (same line)
    // 2. Detect columns by X-coordinate clustering  
    // 3. Identify prices by pattern matching + right alignment
    // 4. Match menu items to prices by proximity
    // 5. Detect categories by font size/style differences
    
    const lines = groupByYCoordinate(textItems);
    const columns = detectColumns(lines);
    const menuItems = extractMenuItemsFromLayout(columns);
    
    return {
      menuItems,
      categories: detectCategories(textItems),
      confidence: calculateLayoutConfidence(menuItems, textItems)
    };
  }
  ```

### OCR Fallback Parser
- **File**: `server/src/services/parsers/pdfParser.ts`
- **Method**: `parseScannedPdf()` - implement using poppler for PDF rasterization
- **Implementation**:
  ```typescript
  async function parseScannedPdf(filePath: string): Promise<ParseResult> {
    // Use poppler to convert PDF to images
    const outputDir = `/tmp/pdf_pages_${Date.now()}`;
    await execAsync(`pdftoppm -png "${filePath}" "${outputDir}/page"`);
    
    // Apply OCR to each page
    const pageFiles = await fs.readdir(outputDir);
    const pageTexts = await Promise.all(
      pageFiles.map(async (file) => {
        const imagePath = path.join(outputDir, file);
        const { data: { text } } = await Tesseract.recognize(imagePath, 'eng');
        return text;
      })
    );
    
    // Combine all page text and parse
    const fullText = pageTexts.join('\n\n--- PAGE BREAK ---\n\n');
    
    // Cleanup temp files
    await fs.rmdir(outputDir, { recursive: true });
    
    return {
      success: true,
      rawOutput: {
        fullText,
        pageTexts,
        ocrMetadata: {
          pageCount: pageTexts.length,
          totalCharacters: fullText.length
        }
      },
      parseMethod: 'pdf_ocr',
      confidence: calculateOCRConfidence(fullText)
    };
  }
  ```

### Parsing vs Analysis Separation
- **Parsing Phase**: Extract raw text/layout data, store in `parse_runs.rawOutput`
  - Layout-aware PDF parsing → structured text with coordinates
  - OCR PDF parsing → raw text extraction  
  - HTML parsing → DOM structure extraction
- **Analysis Phase**: Process raw output into menu items (separate service)
  - Read from `parse_runs.rawOutput`
  - Apply menu item extraction algorithms
  - Store structured results in `analysis_runs.analysisResults`
  - Enable re-analysis without re-parsing/re-OCR

## Phase 5: Public Upload Migration

### Unified Architecture Migration
- **File**: `server/src/api.ts`
- **Endpoint**: `/public/upload-menu` (lines 103-263)
- **Changes**:
  - Replace immediate processing with `documentTriage.ts` + `parseQueue.enqueueDocumentJob()`
  - Use `documents` table instead of dual `menuUploads` + `publicUploads` tracking
  - Maintain same response format for frontend compatibility

### API Compatibility
- **Frontend**: `ui/src/pages/PublicUpload.tsx` and `ui/src/components/UrlUpload.tsx`
- **Requirement**: Ensure existing frontend functionality remains unchanged
- **Response**: Keep same JSON structure but source from unified pipeline

### Legacy Code Removal
- **Files to Modify**:
  - Remove public upload specific logic from `server/src/api.ts`
  - Remove `server/src/schema/publicUploads.ts` table (after migration)
  - Clean up dual processing paths in ParseQueue
- **Database Migration**: Migrate existing data from `publicUploads` to `documents` table

## Technical Dependencies

### Database Schema Changes
- Add `documents` table with Drizzle migration
- Add indexes on `documents.userId`, `documents.status`, `documents.sourceType`
- Migrate existing `restaurantMenuSources` and `publicUploads` data

### Service Integration Points
- `documentTriage.ts` ↔ `parseQueue.ts`: Job enqueueing
- `fileStorage.ts` ↔ `pdfParser.ts`: File retrieval for processing
- `parseRouter.ts` ↔ All parsers: Strategy selection and routing
- `auth.ts` ↔ Document services: User association and permissions

### Error Handling & Retry Logic
- Maintain existing ParseQueue retry mechanism (3 attempts)
- Add file storage error handling and cleanup
- Document corruption detection and appropriate error responses
- User-friendly error messages for frontend display

## Implementation Order

1. **Phase 1**: Immediate testing relief through authenticated upload
2. **Phase 2**: Foundation services (triage, storage, documents table)
3. **Phase 3**: ParseQueue unification and job routing
4. **Phase 4**: Real PDF parsing capabilities
5. **Phase 5**: Public upload migration and cleanup

This phased approach ensures rapid testing capability while building toward a robust, unified architecture.